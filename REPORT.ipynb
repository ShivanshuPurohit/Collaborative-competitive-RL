{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Collaboration and Competition - Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG and MADDPG Algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we use the **DDPG** algorithm (Deep Deterministic Policy Gradient) and the **MADDPG** algorithm,     \n",
    "a wrapper for DDPG. MADDPG stands for **Multi-Agent DDPG**. DDPG is an algorithm which concurrently   learns    \n",
    "a Q-function and a policy.  It uses off-policy data and the Bellman equation to learn the Q-function, \n",
    "and uses    \n",
    "the Q-function to learn the policy. This dual mechanism is the  actor-critic method. The DDPG algorithm uses   \n",
    "two additional mechanisms: _Replay Buffer_ and _Soft Updates_.  \n",
    "\n",
    "In MADDPG, we train two separate agents, and the agents need to **collaborate** (like don’t let the   ball hit the ground)   \n",
    "and **compete** (like gather as many points as possible). Just doing a simple extension of single \n",
    "agent RL    \n",
    "by independently training the two agents does not work very well because the agents are independently updating    \n",
    "their policies as learning progresses. And this causes the   environment to appear non-stationary from the viewpoint   \n",
    "of any one agent. \n",
    "\n",
    "In MADDPG, _each agent’s critic is trained using the observations and actions_ from **both agents** , whereas   \n",
    "each _agent’s actor is trained using just_ its **own observations**.  \n",
    "\n",
    "In the finction _step()_ of the _class madppg_\\__agent_, we collect all current info\n",
    " for **both agents**  into  the **common** variable    \n",
    "_memory_ of the type  _ReplayBuffer_.  Then we get the random _sample_ from _memory_  into the variable _experiance_.   \n",
    "This _experiance_   together with the current number of agent (0 or 1) go to the function _learn()_.   We get the corresponding    \n",
    "agent (of type _ddpg_\\__agent_):\n",
    "\n",
    "      agent = self.agents[agent_number]\n",
    "\n",
    "and _experiance_ is transferred to function _learn()_  of the _class ddpg_\\__agent_.  There, the actor and the critic \n",
    "are handled by different ways.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Eight Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this prohect, there are 8 _neural networks_.  For the training, we \n",
    "create one _maddpg agent_. \n",
    "\n",
    "         maddpg = maddpg_agent()\n",
    "         \n",
    "In turn, _maddpg agent_ creates 2 _ddpg agents_: \n",
    "         \n",
    "         self.agents = [ddpg_agent(state_size, action_size, i+1, random_seed=0) \n",
    "                  for i in range(num_agents)]    \n",
    "\n",
    "Each of two agents (red and blue) create 4 neural networks:\n",
    "\n",
    "        self.actor_local = Actor(state_size, action_size).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size).to(device)\n",
    "\n",
    "        self.critic_local = Critic(state_size, action_size).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size).to(device)\n",
    "\n",
    "Classes Actor and Critic are provided by **model.py**. The typical behavior of the actor \n",
    "\n",
    "        actor_target(state) -> next_actions\n",
    "        actor_local(states) -> actions_pred\n",
    "        \n",
    "see function _learn()_ in maddpg agent. The typical behavior of the critic is as follows:\n",
    "\n",
    "        critic_target(state, action) -> Q-value \n",
    "        -critic_local(states, actions_pred) -> actor_loss\n",
    "        \n",
    "see function _learn()_ in ddpg agent.        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of the actor and critic networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the actor and critic classes implement the neural network\n",
    "with 3 fully-connected layers and 2 rectified nonlinear layers. These networks are realized in the framework\n",
    "of package PyTorch. Such a network is used in Udacity model.py code for the Pendulum model using DDPG.\n",
    "The number of neurons of the fully-connected layers are as follows:\n",
    "\n",
    "for the actor:   \n",
    "Layer fc1, number of neurons: state_size x fc1_units,   \n",
    "Layer fc2, number of neurons: fc1_units x fc2_units,   \n",
    "Layer fc3, number of neurons: fc2_units x action_size,   \n",
    "\n",
    "for the critic:   \n",
    "Layer fcs1, number of neurons: (state_size + action_size) x n_agents x fcs1_units,   \n",
    "Layer fc2, number of neurons: (fcs1_units x fc2_units,   \n",
    "Layer fc3, number of neurons: fc2_units x 1.   \n",
    "\n",
    "Here, state_size = 24, action_size = 2.       \n",
    "The input parameters fc1_units, fc2_units, fcs1_units are all taken = 64.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From **ddpg_agent.py** \n",
    "\n",
    "        GAMMA = 0.99                    # discount factor  \n",
    "        TAU = 5e-2                      # for soft update of target parameters   \n",
    "        LR_ACTOR = 5e-4                 # learning rate of the actor   \n",
    "        LR_CRITIC = 5e-4                # learning rate of the critic  \n",
    "        WEIGHT_DECAY = 0.0              # L2 weight decay   \n",
    "        NOISE_AMPLIFICATION = 1         # exploration noise amplification  \n",
    "        NOISE_AMPLIFICATION_DECAY = 1   # noise amplification decay\n",
    "\n",
    "From **maddpg_agent.py**\n",
    "\n",
    "        BUFFER_SIZE = int(1e6)          # replay buffer size   \n",
    "        BATCH_SIZE = 512                # minibatch size   \n",
    "        LEARNING_PERIOD = 2             # weight update frequency \n",
    "        \n",
    "Note that parameters LEARNING_PERIOD is important. The corresponding code is in the function   _step()_.\n",
    "\n",
    "     if len(self.memory) > BATCH_SIZE and timestep % LEARNING_PERIOD == 0: \n",
    "         for a_i, agent in enumerate(self.agents):\n",
    "              experiences = self.memory.sample()\n",
    "              self.learn(experiences, a_i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my local machine with GPU, the desired average reward **+0.5** was achieved in **2761** episodes in **28** minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episode: 50, Score: -0.0050, \tAverage Score: -0.0050, Time: 00:00:06\n",
    "\n",
    "Episode: 100, Score: -0.0050, \tAverage Score: -0.0050, Time: 00:00:21\n",
    "\n",
    "Episode: 150, Score: -0.0050, \tAverage Score: -0.0050, Time: 00:00:37\n",
    "\n",
    "Episode: 200, Score: -0.0050, \tAverage Score: -0.0050, Time: 00:00:52 \n",
    "\n",
    "Episode: 250, Score: -0.0050, \tAverage Score: -0.0050, Time: 00:01:08\n",
    "\n",
    "Episode: 300, Score: -0.0050, \tAverage Score: -0.0050, Time: 00:01:24\n",
    "\n",
    "Episode: 350, Score: -0.0050, \tAverage Score: -0.0050, Time: 00:01:40 \n",
    "\n",
    "Episode: 400, Score: -0.0050, \tAverage Score: 0.0045, Time: 00:02:04 \n",
    "\n",
    "Episode: 450, Score: -0.0050, \tAverage Score: 0.0095, Time: 00:02:26 \n",
    "\n",
    "Episode: 500, Score: 0.0450, \tAverage Score: 0.0055, Time: 00:02:46 \n",
    "\n",
    "Episode: 550, Score: -0.0050, \tAverage Score: 0.0025, Time: 00:03:03 \n",
    "\n",
    "Episode: 600, Score: -0.0050, \tAverage Score: 0.0005, Time: 00:03:21 \n",
    "\n",
    "Episode: 650, Score: 0.0450, \tAverage Score: 0.0010, Time: 00:03:39 \n",
    "\n",
    "Episode: 700, Score: -0.0050, \tAverage Score: -0.0010, Time: 00:03:55 \n",
    "\n",
    "Episode: 750, Score: -0.0050, \tAverage Score: -0.0020, Time: 00:04:12 \n",
    "\n",
    "Episode: 800, Score: -0.0050, \tAverage Score: 0.0000, Time: 00:04:31 \n",
    "\n",
    "Episode: 850, Score: -0.0050, \tAverage Score: 0.0000, Time: 00:04:48 \n",
    "\n",
    "Episode: 900, Score: -0.0050, \tAverage Score: -0.0010, Time: 00:05:05 \n",
    "\n",
    "Episode: 950, Score: -0.0050, \tAverage Score: 0.0020, Time: 00:05:24 \n",
    "\n",
    "Episode: 1000, Score: -0.0050, \tAverage Score: 0.0035, Time: 00:05:43 \n",
    "\n",
    "Episode: 1050, Score: -0.0050, \tAverage Score: 0.0025, Time: 00:06:01 \n",
    "\n",
    "Episode: 1100, Score: -0.0050, \tAverage Score: 0.0020, Time: 00:06:20 \n",
    "\n",
    "Episode: 1150, Score: -0.0050, \tAverage Score: 0.0025, Time: 00:06:39 \n",
    "\n",
    "Episode: 1200, Score: -0.0050, \tAverage Score: 0.0020, Time: 00:06:58 \n",
    "\n",
    "Episode: 1250, Score: 0.0450, \tAverage Score: 0.0040, Time: 00:07:18 \n",
    "\n",
    "Episode: 1300, Score: -0.0050, \tAverage Score: 0.0065, Time: 00:07:39 \n",
    "\n",
    "Episode: 1350, Score: 0.0450, \tAverage Score: 0.0180, Time: 00:08:09 \n",
    "\n",
    "Episode: 1400, Score: -0.0050, \tAverage Score: 0.0210, Time: 00:08:31 \n",
    "\n",
    "Episode: 1450, Score: -0.0050, \tAverage Score: 0.0235, Time: 00:09:05 \n",
    "\n",
    "Episode: 1500, Score: -0.0050, \tAverage Score: 0.0215, Time: 00:09:26 \n",
    "\n",
    "Episode: 1550, Score: -0.0050, \tAverage Score: 0.0050, Time: 00:09:44 \n",
    "\n",
    "Episode: 1600, Score: -0.0050, \tAverage Score: 0.0055, Time: 00:10:06 \n",
    "\n",
    "Episode: 1650, Score: 0.0450, \tAverage Score: 0.0130, Time: 00:10:31 \n",
    "\n",
    "Episode: 1700, Score: -0.0050, \tAverage Score: 0.0120, Time: 00:10:52 \n",
    "\n",
    "Episode: 1750, Score: -0.0050, \tAverage Score: 0.0075, Time: 00:11:13 \n",
    "\n",
    "Episode: 1800, Score: 0.0450, \tAverage Score: 0.0100, Time: 00:11:36 \n",
    "\n",
    "Episode: 1850, Score: -0.0050, \tAverage Score: 0.0060, Time: 00:11:54 \n",
    "\n",
    "Episode: 1900, Score: -0.0050, \tAverage Score: -0.0005, Time: 00:12:12 \n",
    "\n",
    "Episode: 1950, Score: -0.0050, \tAverage Score: 0.0115, Time: 00:12:40 \n",
    "\n",
    "Episode: 2000, Score: -0.0050, \tAverage Score: 0.0190, Time: 00:13:03 \n",
    "\n",
    "Episode: 2050, Score: -0.0050, \tAverage Score: 0.0210, Time: 00:13:34 \n",
    "\n",
    "Episode: 2100, Score: 0.0450, \tAverage Score: 0.0190, Time: 00:13:57 \n",
    "\n",
    "Episode: 2150, Score: -0.0050, \tAverage Score: 0.0215, Time: 00:14:28 \n",
    "\n",
    "Episode: 2200, Score: 0.0950, \tAverage Score: 0.0605, Time: 00:15:22 \n",
    "\n",
    "Episode: 2250, Score: 0.0450, \tAverage Score: 0.0705, Time: 00:16:00 \n",
    "\n",
    "Episode: 2300, Score: 0.0450, \tAverage Score: 0.0550, Time: 00:16:40 \n",
    "\n",
    "Episode: 2350, Score: 0.0450, \tAverage Score: 0.0670, Time: 00:17:29 \n",
    "\n",
    "Episode: 2400, Score: 0.0450, \tAverage Score: 0.0720, Time: 00:18:14 \n",
    "\n",
    "Episode: 2450, Score: 0.2450, \tAverage Score: 0.0680, Time: 00:19:01 \n",
    "\n",
    "Episode: 2500, Score: 0.0450, \tAverage Score: 0.0720, Time: 00:19:50 \n",
    "\n",
    "Episode: 2550, Score: 0.1450, \tAverage Score: 0.0670, Time: 00:20:31 \n",
    "\n",
    "Episode: 2600, Score: -0.0050, \tAverage Score: 0.0670, Time: 00:21:19 \n",
    "\n",
    "Episode: 2650, Score: 0.1450, \tAverage Score: 0.0890, Time: 00:22:19 \n",
    "\n",
    "Episode: 2700, Score: 0.0950, \tAverage Score: 0.1065, Time: 00:23:24 \n",
    "\n",
    "Episode: 2750, Score: 2.6000, \tAverage Score: 0.3097, Time: 00:27:27 \n",
    "\n",
    "*** Environment solved in 2761 episodes!\tAverage Score: 0.52 ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check different values for hyperparameters such as LEARNING_PERIOD, and neural network parameters fc1_units, fc2_units, etc.\n",
    "2. How does the addition of new nonlinear layers in the used neural networks affect the robustness of the algorithm.\n",
    "3. It would be interesting to train agents using [MAPPO](https://github.com/kotogasy/unity-ml-tennis) and compare them with MADDPG. \n",
    "4. Running the agent for more episodes should also improve the score, since we see a very sharp increase in the average score by the end of training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
